<!DOCTYPE html>
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>
		Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360° Videos
	</title>
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="static/css/styles.css">
	<link rel="stylesheet" href="static/css/project-page.css">


	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<body>
    <center>
		<table style="width:100%">
				<tr>
					<center>
						<h1 style="font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; font-size:300%; color: #003971">
							SalViT360-AV
						</h1>
						<h2 style="font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; font-size:150%; color: #003971;">
							Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360° Videos
						</h2>
					</center>
				</tr>

				<tr>
					<center>
						<div class="authors">
							<a href="https://github.com/MertCokelek/" style="margin-right: 10px;">
								Mert Cokelek
							</a>
							<a href="https://corupta.net" style="margin-right: 10px;">
								Halit Ozsoy
							</a>
							<a href="https://nevrez.github.io/" style="margin-right: 10px;">
								Nevrez Imamoglu
							</a>
							<a href="https://cagriozcinar.netlify.app/" style="margin-right: 10px;">
								Cagri Ozcinar
								<!-- <sup>2</sup> -->
							</a>
							<a href="https://psychology.bogazici.edu.tr/İnciAyhan" style="margin-right: 10px;">
								Inci Ayhan
							</a>
							<a href="https://web.cs.hacettepe.edu.tr/~erkut/" style="margin-right: 10px;">
								Erkut Erdem
								<!-- <sup>3</sup> -->
							</a>
							<a  href="https://aykuterdem.github.io"    
								style="margin-right: 10px;">
								Aykut Erdem
								<!-- <sup>4</sup> -->
							</a>
							<br>
						</div>
						<tr bgcolor="white" style="border-color: white;">
							<center>
								<div class="logos" style="scale: 75%;">
									<ul>
											<li><img src="./static/imgs/kuis-ai.png" 		style="margin-left:-210px; width: 20%;"></li>
											<li><img src="./static/imgs/boun-vr-logo.png" 	style="margin-left:-20px;  width: 20%;"></li>
											<li><img src="./static/imgs/aist-logo.jpg" 		style="margin-left:10px;  width: 14%;"></li>
											<li><img src="./static/imgs/msk-logo.svg" 		style="margin-left:20px;  width: 14%;"></li>
											<li><img src="./static/imgs/boun-logo.png" 		style="margin-left:30px; width: 10%;"></li>
											<li><img src="./static/imgs/haceteppe-logo.png" style="margin-left:35px;  width: 17%;"></li>
											<li><img src="./static/imgs/koc-logo.svg" 		style="margin-left:40px; width: 20%;"></li>

										</ul>
								</div>
							</center>
						</tr>
					</center>
				</tr>
			
				<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
					Abstract
				</div>
				<br>

				<p class="paragraph desc" style="padding: 0% 0% 0% 0%;">
					Omnidirectional videos (ODVs) are redefining viewer experiences in virtual reality (VR) by offering an unprecedented full field-of-view (FOV). 
					This study extends the domain of saliency prediction to 360◦ environments, addressing the complexities of spherical distortion and the integration of spatial audio. 
					Contextually, ODVs have transformed user experience by adding a spatial audio dimension that aligns sound direction with the viewer’s perspective in spherical scenes. 
					Motivated by the lack of comprehensive datasets for 360◦ audio-visual saliency prediction, our study curates YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying audio-visual conditions. 
					Our goal is to explore how to utilize audio-visual cues to effectively predict visual saliency in 360◦ videos. 
					Towards this aim, we propose two novel saliency prediction models: 
					SalViT360, a vision-transformer-based framework for ODVs equipped with spherical geometry-aware spatio-temporal attention layers, and SalViT360-AV, which further incorporates transformer adapters conditioned on audio input. 
					Our results on a number of benchmark datasets, including our YT360-EyeTracking, demonstrate that SalViT360 and SalViT360-AV significantly outperform existing methods in predicting viewer attention in 360◦ scenes. 
					Interpreting these results, we suggest that integrating spatial audio cues in the model architecture is crucial for accurate saliency prediction in omnidirectional videos.
				</br>
				</p>

				<tr>
					<div style="width: 50;"> 
						<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
							Model Overview
						</div>
					<br>
					<img src="./static/imgs/AV-overview-2.png"style="padding-bottom: 3%;padding-top: 4%">
					<p class="paragraph desc">
						<b>Overview of the proposed SalViT360-AV pipeline.</b>        
						We use the SalViT360 [1] model as the video saliency module (top), and our implementation allows for any audio model as the audio backbone (bottom).        
						The audio stream takes input spatial audio waveforms \(x_{aud} \in \mathbb{R}^{4 \times N}\) encoded as first-order ambisonics in 4-channel B-format.        
						To simulate what the subjects are hearing while looking at a particular location, we rotate the ambisonics depending on the angular coordinates \((\theta, \phi)\) for each tangent viewport \(\{x_t\}_{T}^{t=1}\) (1).        
						The rotated waveforms are mono, which enables us to use any pre-trained audio backbone for feature extraction (2).        
						The extracted features are passed to the adapter layers in each upgraded transformer block (3) for audio-visual tuning.        
						While the total number of parameters in the video pipeline is 37M, the additional adapter layers require only 600k parameters for fine-tuning. 
					</p>
					<br>
				</tr>
				<tr>
					<div style="width: 100%;"> 
						<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
							<a href="https://yt360-eye-tracking.corupta.net/">YT360-EyeTracking Dataset</a>
						</div>
						<p class="paragraph desc" style="text-align: center;"></p>
						We curated a subset of 360° videos with first order ambisonics from YT-360 Dataset 
						containing equal amounts of clips from each scene type (indoor, outdoor-natural, and outdoor-manmade) and audio type (human speech, music instrument, and vehicle sounds). 
						Each category combination has 9 clips, totaling 81 clips.
					</p>
					<a href="https://yt360-eye-tracking.corupta.net/">
						<img src="./static/imgs/yt360-eyetracking-overview.png" style="padding-bottom: 3%;">
					</a>


					</div>
					<br>
					<div style="width: 100%;"> 
						<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
							<a href="https://drive.google.com/drive/folders/1bsC_4vYVrLnWCmCGtSs4aiiIKmEuuJed?usp=drive_link">Qualitative Results</a>
						</div>
					<br>

					</div>
					<div class="video-container">
						<video controls>
							<source src="./static/media/supp-videos/supp-RI2eIMZdyX8-1063.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>

						<video controls>
							<source src="./static/media/supp-videos/supp-RGzcsBqIbPk-969.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>

						<video controls>
							<source src="./static/media/supp-videos/supp-UgyKRGwMm7U-72.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>

						<video controls>
							<source src="./static/media/supp-videos/supp-Wb_RGCFKpDw-745.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
						<video controls>
							<source src="./static/media/supp-videos/supp-xyccHcWoS74-138.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
				</tr>

					<br>
					<p class="paragraph desc" style="text-align: center;"></p>
						Qualitative comparison of <b>Ground Truth</b> saliency maps and <b>SalViT360-AV</b> predictions on 
							<a href="https://yt360-eye-tracking.corupta.net/"><b>YT360-EyeTracking</b></a>
						 	and 
							<a href="https://github.com/cozcinar/360_Audio_Visual_ICMEW2020"><b>360AV-HM</b></a>
							datasets.
						 	<br>
							Click <a href="https://drive.google.com/drive/folders/1bsC_4vYVrLnWCmCGtSs4aiiIKmEuuJed?usp=drive_link"><b>here</b></a> for more qualitative results.
					</p>
				
				<table style="width:100%">
					<tr bgcolor="white"; style="border-color: white;">
						<td bgcolor="white"; style="border-color: white;">
							<tr bgcolor="white"; style="border-color: white;">
								<div class="icons" style="padding: 0% 0% 0% 0%;">
									<table bgcolor="white" style="width: 50%; border: none;border-collapse: collapse; border-spacing:0;" cellspacing="0">
										<tr>
											<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
												Additional Links
											</div>
											<br>
											<center>
												<td align="center" style="border: none; width: 20%;">
													<a 
													href="https://arxiv.org/abs/2308.13004">
														<i class="fa fa-file-pdf-o" style="font-size:36px;"></i> <br>
														SalViT360
													</a>
												</td>
												<td align="center" style="border: none; width: 20%;">
													<a 
													href="static/docs/supp.pdf">
														<i class="fa fa-file-pdf-o" style="font-size:36px;"></i> <br>
														SalViT360 Appendix
													</a>
												</td>
												<td align="center" style="border: none; width: 20%">
													<a href="https://github.com/MertCokelek/SalViT360/">
														<i class="fa fa-github" style="font-size:36px"></i>
														<br>
														Code
													</a>
												</td>
											</center>
										</tr>
									</table>

								</div>

							</tr>
							<tr>
								<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
									BibTeX
								</div>
								<div class="section bibtex" ; align="left" style="width: 70%;">
									<br>

									<pre>
	@misc{cokelek2023spherical,
		title={Spherical Vision Transformer for 360-degree Video Saliency Prediction}, 
		author={Mert Cokelek and Nevrez Imamoglu and Cagri Ozcinar and Erkut Erdem and Aykut Erdem},
		year={2023},
		eprint={2308.13004},
		archivePrefix={arXiv},
		primaryClass={cs.CV}
	}
										</pre>  

								</div>
							</tr>
						</td>
					</tr>
				</table>

			</tbody>

			<!-- </table> -->
 	</div>

	<tr>
		<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
			Contact
		</div>		
		<br>
		For any questions, please contact Mert Cokelek at 
		<a href="mailto:mcokelek21@ku.edu.tr">mcokelek21@ku.edu.tr</a> or Halit Ozsoy at <a href="mailto:halitozsoy1584@gmail.com">halitozsoy1584@gmail.com</a>.


	</tr>
	
</body>
</html>
