<!DOCTYPE html>
<html>
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>
		Spherical Vision Transformers for 360◦ Video Saliency Prediction

	</title>
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="static/css/styles.css">
	<link rel="stylesheet" href="static/css/project-page.css">


	<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<body>
    <center>
		<table style="width:100%">
				<tr>
					<center>
						<h1 style="font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; font-size:300%; color: #003971">
							SalViT360
						</h1>
						<h2 style="font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; font-size:150%; color: #003971;">
							Spherical Vision Transformers for 360◦ Video Saliency Prediction 
						</h2>
					</center>
				</tr>

				<tr>
					<center>
						<div class="authors">
							<a href="https://github.com/MertCokelek/" style="margin-right: 10px;">
								Mert Cokelek
							</a>
							<a href="https://nevrez.github.io/" style="margin-right: 10px;">
								Nevrez Imamoglu
							</a>
							<a href="https://cagriozcinar.netlify.app/" style="margin-right: 10px;">
								Cagri Ozcinar
								<!-- <sup>2</sup> -->
							</a>
							<a href="https://web.cs.hacettepe.edu.tr/~erkut/" style="margin-right: 10px;">
								Erkut Erdem
								<!-- <sup>3</sup> -->
							</a>
							<a  href="https://aykuterdem.github.io"    
								style="margin-right: 10px;">
								Aykut Erdem
								<!-- <sup>4</sup> -->
							</a>
							<br>
						</div>
						<tr bgcolor="white" style="border-color: white;">
							<center>
								<div class="logos" style="scale: 70%;">
									<ul>
										<li><img src="./static/imgs/koc-logo.svg" style="margin-left:-40px; width: 20%;"></li>
										<li><img src="./static/imgs/kuis-ai.png" style="width: 20%;"></li>
										<li><img src="./static/imgs/aist-logo.jpg" style="width: 20%;"></li>
										<li><img src="./static/imgs/msk-logo.png" style="width: 20%;"></li>
										<li><img src="./static/imgs/haceteppe-logo.png" style="width: 17%;"></li>
									</ul>
								</div>
							</center>
						</tr>
					</center>
				</tr>
			
				<tr>
					<div style="width: 85%;"> 
						<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
							Overview
						</div>
					<br>

						<!-- <hr style="width: 100%;"> -->
					<img src="./static/imgs/overview.png"style="padding-bottom: 3%;">
					<p class="paragraph desc">
						<b>Overview of the proposed SalViT360 model.</b> 
						The ERP video clip of F frames (1) is projected to F × T tangent images per set (2). 
						Each tangent image is encoded and fused with spherical-geometry-aware position embeddings (3) for the 360◦ video transformer to aggregate global information (4). 
						The outputs are decoded into saliency predictions in tangent space (5), 
						which are projected back to ERP, giving the final saliency map (6). 
						In addition to the supervised loss, the model is trainable with L<sub>VAC(P, P′)</sub> 
						to minimize the tangent artefacts (7). 
						During test time, the model works with a single tangent set. 
						For simplicity, only one set of tangent images is shown.
					</p>
					
					</div>

					<br>
					<br>
					<iframe width="920" height="240" src="./static/media/outputs_webpgage/025_grid.mp4"></iframe>
					<iframe width="920" height="240" src="./static/media/outputs_webpgage/MzcdEI-tSUc_grid.mp4"></iframe>
					<div class="row">
						<div class="column">
							
							<h4 style="margin-left: 25%; text-align: center; font-family: Geneva, Verdana, sans-serif; color: #003971">
								SalViT360
							</h4>
						</div>
						<div class="column">
							<h4 style="margin-left: -25%; text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
								Ground Truth
							</h4>
						</div>
					</div>
				</tr>

				<div style="width:85%">
					<br>
					<p class="paragraph desc">
						<b>Qualitative comparison</b> of <i>SalViT360 predictions</i> and <i>ground truth</i> saliency maps on 
							<a href="https://github.com/xuyanyu-shh/VR-EyeTracking">VR-EyeTracking</a>
						 	and 
							<a href="https://github.com/cozcinar/360_Audio_Visual_ICMEW2020">360AV-HM</a>
							datasets.
					</p>
				</div>
				
				<table style="width:80%">
					<tr bgcolor="white"; style="border-color: white;">
						<td bgcolor="white"; style="border-color: white;">
							<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
								Abstract
							</div>
							<br>

							<p class="paragraph desc" style="padding: 0% 0% 0% 0%;">
								The growing interest in omnidirectional videos (ODVs) that capture the full field-of-view (FOV) has gained 360◦ saliency prediction importance in computer vision. 
								However, predicting where humans look in 360◦ scenes presents unique challenges, including spherical distortion, high resolution, and limited labelled data. 
								To address these challenges, we propose a novel vision-transformer-based model for omnidirectional videos named SalViT360 that leverages tangent image representations. 
								We introduce a spherical geometry-aware spatio-temporal self-attention mechanism that is capable of effective omnidirectional video understanding. 
								Furthermore, we present a consistency-based unsupervised regularization term for projection-based 360◦ dense-prediction models to reduce artefacts in the predictions that occur after inverse projection. 
								Our approach is the first to employ tangent images for omnidirectional saliency prediction, 
								and our experimental results on three ODV saliency datasets demonstrate its effectiveness compared to the state-of-the-art.</br>
							</p>

							<tr bgcolor="white"; style="border-color: white;">

								<div class="icons" style="padding: 0% 20% 0% 20%;">
									<table bgcolor="white" style="width: 50%; border: none;border-collapse: collapse; border-spacing:0;" cellspacing="0">
										<tr>
											<center>
												<td align="center" style="border: none; width: 20%;">
													<a 
													href="https://arxiv.org/abs/2308.13004">
														<i class="fa fa-file-pdf-o" style="font-size:36px;"></i> <br>
														Arxiv
													</a>
												</td>
												<td align="center" style="border: none; width: 20%;">
													<a 
													href="static/docs/supp.pdf">
														<i class="fa fa-file-pdf-o" style="font-size:36px;"></i> <br>
														Supplementary
													</a>
												</td>

												<td align="center" style="border: none; width: 20%">
													<a href="https://github.com/MertCokelek/SalViT360/">
														<i class="fa fa-github" style="font-size:36px"></i>
														<br>
														Code
													</a>
												</td>
											</center>
										</tr>
									</table>

								</div>

							</tr>
							<tr>
								<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
									BibTeX
								</div>
								<div class="section bibtex" ; align="left" style="width: 70%;">
									<pre>
@misc{cokelek2023spherical,
	title={Spherical Vision Transformer for 360-degree Video Saliency Prediction}, 
	author={Mert Cokelek and Nevrez Imamoglu and Cagri Ozcinar and Erkut Erdem and Aykut Erdem},
	year={2023},
	eprint={2308.13004},
	archivePrefix={arXiv},
	primaryClass={cs.CV}
	}
										</pre>  

								</div>
							</tr>
						</td>
					</tr>
				</table>

			</tbody>

			<!-- </table> -->
 	</div>

	<tr>
		<div class="section-title" style="text-align: center; font-family:'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif; color: #003971">
			Contact
		</div>		
		For any questions, please contact Mert Cokelek at <a href="mailto:mcokelek21@ku.edu.tr">mcokelek21@ku.edu.tr.


	</tr>
	
</body>
</html>
