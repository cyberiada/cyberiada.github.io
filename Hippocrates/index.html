<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="Lorem ipsum"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Hippocrates</title>
  <link rel="icon" href="./static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="static/js/jquery.min.js"></script>
  <script src="static/js/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
  <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
  <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
  <style>
      /* #special-table tbody tr td:nth-child(0),
      #special-table tbody tr td:nth-child(1) {
          padding-right: 30px;
      }
      #special-table tbody tr td:nth-child(0),
      #special-table tbody tr td:nth-child(1) {
          padding-left: 30px;
      } */

      .number-box {
          border: 1px solid #000; /* ÈªëËâ≤ËæπÊ°Ü */
          padding: 3px; /* ÂÜÖËæπË∑ù */
          margin: 3px; /* Â§ñËæπË∑ù */
      }
  </style>

    
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <style>
                  .logo {
                    width: 1.5em; /* Ë∞ÉÊï¥ÂõæÊ†áÂ§ßÂ∞è */
                    position: relative; /* ‰Ωø top Âíå left Â±ûÊÄßÁîüÊïà */
                    top: -10px; /* Âêë‰∏äÁßªÂä® */
                    left: -5px; /* ÂêëÂ∑¶ÁßªÂä® */
                    vertical-align: middle;
                  }
                </style>
                
                <img src="static/images/logo.png" class="logo" alt="Logo" />
                Hippocrates:<br>An Open-Source Framework for Advancing<br>Large Language Models in Healthcare
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><sup>1,2</sup><a href="https://emrecanacikgoz.github.io/" target="_blank">Emre Can Acikgoz</a>,</span>
              <span class="author-block"><sup>1,2</sup><a href="" target="_blank">Osman Batur ƒ∞nce</a>,</span>
              <span class="author-block"><sup>4</sup><a href="" target="_blank">Rayene Bech</a>,</span>
              <span class="author-block"><sup>5</sup><a href="" target="_blank">Arda Anƒ±l Boz</a>,</span>
              <span class="author-block"><sup>1</sup><a href="" target="_blank">Ilker Kesen</a>,</span><br>
              <span class="author-block"><sup>1,2</sup><a href="https://aykuterdem.github.io/" target="_blank">Aykut Erdem</a>,</span>
              <span class="author-block"><sup>1,3</sup><a href="https://web.cs.hacettepe.edu.tr/~erkut/" target="_blank">Erkut Erdem</a></span>
            </div>

            <div class="is-size-5 publication-authors">
                <span class="author-block">
                <sup>1</sup>Ko√ß University, KUIS AI Center,
                <sup>2</sup>Ko√ß University, Department of Computer Engineering,
                <sup>3</sup>Hacettepe University, Department of Computer Engineering,
                <sup>4</sup>Yƒ±ldƒ±z Technical University, Department of Computer Engineering,
                <sup>5</sup>Robert College
            </div>

            <div class="column has-text-centered">
              <span class="author-block">
                  <a href="mailto:eacikgoz17@ku.edu.tr">eacikgoz17@ku.edu.tr</a></span>
            </div>

            <header style="text-align: center;">
              <img src="static/images/hippo2.png" alt="ResultsTable" width="85%" >
            </header>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- ArXiv abstract Link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.16621" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon"> <i class="ai ai-arxiv"></i> </span>
                          <span>arXiv</span>
                        </a>
                      </span>

                      <!-- Huggingface link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/emrecanacikgoz" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">ü§ó</span>
                          <span>Models</span>
                      </a>
                    </span>

                      <span class="link-block">
                        <a href="https://drive.google.com/drive/folders/11tX_nv8dxkZzt9NAMr2t7kiJ1YPvOZUl?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                            <span class="icon"><i class="fas fa-database"></i></span>
                            <span>Datasets</span>
                        </a>
                    </span>

                    <!-- Poster PDF link -->
                    <span class="link-block">
                      <a href="static/images/hippocrates-poster.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>

                    <span class="link-block">
                        <a href="https://github.com/emrecanacikgoz/lm-evaluation-harness" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon"> <i class="fab fa-github"></i> </span>
                            <span>Benchmark</span>
                        </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/emrecanacikgoz/Medical-Factory" target="_blank" class="external-link button is-normal is-rounded is-dark">
                        <span class="icon"> <i class="fab fa-github"></i> </span>
                        <span>Code</span>
                      </a>
                    </span>

                    <!-- Wandb -->
                    <span class="link-block">
                      <a href="https://huggingface.co/emrecanacikgoz" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span>Wandb (Soon)</span>
                      </a>
                    </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<header style="text-align: center;">
  <img src="static/images/fig1.png" alt="ResultsTable" width="70%" >
</header>
-->


<!-- Paper abstract -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                      <p>
                          The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.
                      </p>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Hippo Series -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Hippocrates</h2>
                  <div class="content has-text-justified">
                    <img src="static/images/evaluation-new.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <br>
                    <p>Our release includes:</p>
                        <ul style="list-style-type: disc; margin-left: 20px;">
                            <li> <a href="https://arxiv.org/pdf/2404.16621">Hippocrates</a>, a comprehensive and open-source framework tailored for the medical domain. </li>
                            <li> We provide openly available datasets and establish an intuitive benchmark using the <a href="https://github.com/emrecanacikgoz/lm-evaluation-harness">LM-Evaluation-Harness</a> tool. </li>
                            <li> We also introduce <a href="https://huggingface.co/emrecanacikgoz/hippollama">Hippo-</a><img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/> and <a href="https://huggingface.co/emrecanacikgoz/hippomistral">Hippo-</a><img src="static/images/mistral.png" class="logo2" alt="Logo"  style="width: 18px; height: auto; vertical-align: middle;"/>, two 7B models demonstrating superior performance. </li>
                        </ul>
                    <p> We argue that the development of a broad, varied collection of open models is crucial for deepening our knowledge of language models and enhancing their applicability across various domains.  Our work makes substantial contributions to the field by combining in-depth empirical research with a structured training methodology, offering invaluable insights and tools for future research not only in healthcare but in any area requiring domain-specific adaptation of LLMs. </p>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End Hippo Series -->

<!-- Hippocrates Framework
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Hippocrates Framework</h2>
                  <div class="content has-text-justified">
                    <img src="static/images/fig2.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 95%;"/>
                    <br>
                    <ul style="list-style-type: none;">
                      <li>In this work, we provide full access to our framework, from the data sources to the training configurations and the reproducible evaluation protocols. We conduct a detailed empirical analysis to identify the impact of various design elements on LLM performance, leading to a domain-adapted framework that demonstrates superior performance on multiple medical benchmarks. Based on these insights, we develop a step-by-step guide for the efficient training of medical-LLMs. Our research efforts yield two advanced 7B parameter models, Hippo-Mistral and Hippo-LLaMA. As shown in first figure, our models not only outperform existing 7B and 13B models by a significant margin but also deliver results on par with, and in some cases exceeding, those of 70B models. We argue that the development of a broad, varied collection of open models is crucial for deepening our knowledge of language models and enhancing their applicability across various domains.</li>
                      <li>The figure above shows the evolution of medical LLM performances on the MedQA dataset. Our 7B Hippo-Mistral and Hippo-LLaMA models achieve 50.8% and 59.9% 5-shot accuracy, respectively. Hippo-Mistral outperforms all existing open models, including even those with 70B parameters.</li>
                    </ul> 
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
End Hippocrates Framework -->


<!-- Dataset 
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Datasets</h2>
                  <div class="content has-text-justified">
                    <ul style="list-style-type: none;">
                          <li><strong>Continued Pre-training Data.</strong> A key aspect of our methodology is the integration of specialized medical knowledge through an extensive pre-training corpus, assembled from three specialized datasets: Medical Guidelines, PMC-Patients, and PubMedQA-contexts. The Medical Guidelines dataset comprises clinical practice guidelines, is used for training Meditron models. The PMC-Patients dataset consists of patient summaries extracted from case reports within PubMed Central (PMC). Additionally, the PubMedQA-contexts dataset is constructed by extracting the context field of each sample in the training split of the benchmark PubMedQA. This extensive corpus, consisting of roughly 300M training tokens, forms the foundation of our models, ensuring their proficiency in navigating medical terminology and practices. We systematically assessed the impact of each dataset, both individually and in combination, to optimize our model's performance. <br>
                          <li><strong>General Instructions Data.</strong> This dataset aggregates more than 400K samples from nine different datasets, each derived from the instruction corpora of previous studies. By excluding data from the training or test splits of downstream QA benchmarks, we aim to minimize bias and improve the model's generalization capabilities across different reasoning tasks. A pre-processing protocol was employed to remove irrelevant noise such as superfluous words and web URLs, ensuring the data's quality and relevance.
                          <li><strong>Evaluation Instructions Data.</strong> This dataset was formed to examine the effects of including instruction samples directly from downstream tasks, a common practice in existing studies. Instruction-response pairs were crafted using the training splits of various benchmarks, following the templates established in Meditron. We conducted a series of experiments to assess the distinct influence of each split on each task, both individually and collectively.
                          <li><strong>Medical Preference Data.</strong> Constructing a preference dataset typically involves generating diverse responses to identical queries using LLMs, which are subsequently evaluated by human annotators to identify the most accurate response. This method, however, can become prohibitively expensive, both in terms of computation for generating responses and the financial and time investments required for manual annotation. To circumvent these issues, we leveraged the iCliniq-10k dataset, containing 10K authentic patient-doctor dialogues from icliniq.com. Each dialogue features a patient question accompanied by three different answers: one from an actual doctor, and the others from ChatGPT and ChatDoctor. We conducted a thorough preprocessing of this dataset to eliminate any irrelevant or extraneous information.
                          <li><strong>Medical RLAIF.</strong> To reduce annotation costs, we adopted the RLAIF methodology in the medical domain for the first time. Utilizing detailed prompts based on patient inquiries from the iCliniq-10k dataset, we used GPT4 to determine the optimal response based on predefined instructions. These instructions were derived from those used in qualitative assessments by medical professionals in Med-PaLM, with minor modifications. This annotation approach amounted to a cost of $120.
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
 End Dataset -->
 
<!-- Methodology -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Hippocrates Framework</h2>
                  <div class="content has-text-justified">
                    <img src="static/images/fig2.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 95%;"/>
                    <br>
                    <ul style="list-style-type: none;">
                        <li><strong>Approach.</strong> Our training strategy includes several phases: injection of medical knowledge through continued pre-training, domain-specific instruction tuning, and reinforcement learning from AI-generated feedback for improved alignment with medical experts. Employing the <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA Factory</a> framework, we adhere to replicable and high-performance training standards. Moreover, we adopt the <a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation (LoRA)</a> technique for training efficiency and precision. LoRA enhances LLMs by selectively updating weights within additional trainable layers, thereby accelerating the training process, minimizing memory usage, and mitigating overfitting and catastrophic forgetting. Our foundational models, <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf">LLaMA2 7B</a> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1">Mistral 7B</a>, are selected based on their robust performance across medical benchmarks, demonstrating their capacity to excel without extensive training modifications. <br> <br>
                        <li><strong>Continued Pre-training.</strong> A key aspect of our methodology is the integration of specialized medical knowledge through an extensive pre-training corpus, assembled from three specialized datasets: <a href="https://huggingface.co/datasets/epfl-llm/guidelines">Medical Guidelines</a>, <a href="https://huggingface.co/datasets/zhengyun21/PMC-Patients">PMC-Patients</a>, and <a href="https://huggingface.co/datasets/qiaojin/PubMedQA">PubMedQA-contexts</a>. This stage employs traditional language modeling, focusing on next-token prediction. We systematically assessed the impact of each dataset, both individually and in combination, to optimize our model's performance. <br> <br>
                        <li><strong>Supervised Finetuning.</strong> After continued pre-training, models undergo fine-tuning with an Instruction Tuning (IT) dataset to closely mirror medical directives, aligning model outputs with clinical requirements. We have tested with the different datasets and found that MedQA-train IT works better than the other options. <br> <br> 
                        <li><strong>Medical Preference Data and RLAIF.</strong> To create a preference dataset, we used LLMs to generate varied responses to queries, reviewed by human annotators for accuracy. This method, costly in computation and manual review, led us to use the <a href="https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view">iCliniq-10k</a> dataset containing 10,000 real patient-doctor dialogues. Each includes a patient question with three responses: one from a real doctor and two from AI models. We preprocessed this dataset to remove irrelevant details and applied the <a href="https://arxiv.org/abs/2309.00267">RLAIF</a> methodology in the medical domain for cost-effective annotation, using GPT4 and modified medical assessment guidelines. This approach cost $120. <br> <br>
                        <li><strong>Medical Preference Learning.</strong>  Finally, the instruction-tuned models are further trained with a recent and popular technique called <a href="https://arxiv.org/abs/2305.18290">direct preference optimization (DPO)</a>. In DPO, reinforcement learning is bypassed which allows for direct optimization based on preference data. Unlike RLHF, the responses in DPO need not be derived from the LLM being optimized. Central to DPO is the development of a loss function that evaluates the likelihood of a preferred response over a less preferred one, steering the LLM towards this goal. This makes DPO more stable and significantly reduces computational demands. Our medical LLMs underwent further refinement with these clinical preferences through DPO trainings. <br> <br>

                        The outcome of all this are our medical LLMs, named Hippo-<img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/> and Hippo-<img src="static/images/mistral.png" class="logo2" alt="Logo"  style="width: 18px; height: auto; vertical-align: middle;"/>, built upon the pre-trained LLaMA2 7B and Mistral 7B models. These models were refined through a comprehensive process that included continued pre-training and/or instruction tuning using our carefully curated medical datasets. Following this, we also explored the impact of aligning the models with clinical preferences by conducting further training on medical preference data.
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End Methodology -->

<!-- Stages -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Contribution of Each Training Stage</h2>
                  <div class="content has-text-justified">
                    <img src="static/images/stages.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 95%;"/>
                    <br>
                    <ul style="list-style-type: none;">
                        <li><strong>Hippo-<img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/>.</strong> Our evaluation methodology for the LLaMA2 7B model covers successive training stages: Continued Pre-training (CP), Instruction Tuning (SFT), and Direct Preference Optimization (DPO). As listed in table above, the base model LLaMA2 7B initially achieves an average accuracy of 34.0 across benchmarks. The CP stage marginally increases accuracy to 34.4, indicating initial benefits from domain-focused continued pre-training. The subsequent introduction of SFT yields a substantial performance boost to an average accuracy of 50.3, demonstrating the critical role of customized instruction in enhancing the model‚Äôs capabilities in understanding and answering medical queries. Integrating CP with SFT further improves this performance to 53.0, highlighting the combined value of domain knowledge and specific instruction tuning. The final DPO stage slightly decreases the model's performance to 52.5, albeit with a slight increase in accuracy for MedMCQA and PubMedQA, illustrating DPO's refined impact on model preference alignment. This sequence delineates the incremental enhancements attributable to each training phase, with SFT marking a pivotal improvement. The composite model, LLaMA2 + CP + SFT, is thus designated as Hippo-<img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/> for its distinguished performance across our benchmarks. <br> <br>
                        <li><strong>Hippo-<img src="static/images/mistral.png" class="logo2" alt="Logo"  style="width: 18px; height: auto; vertical-align: middle;"/>.</strong> Following the approach for Hippo-<img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/>, the training evolution for the Mistral 7B model reveals gradual improvement in the model's proficiency in medical question-answering. Initial results from the baseline Mistral 7B model, as shown in the table, show an average benchmark accuracy of 39.3. Implementing CP slightly improves this to 41.0, reflecting the positive yet modest impact of domain-specific continued pre-training. The pivotal SFT stage significantly raises the performance, achieving an average accuracy of 61.6, emphasizing the critical role of customized instruction in enhancing the model's interpretative and response capabilities for medical inquiries. Interestingly, combining CP and SFT results in a slight reduction to 61.1, suggesting a complex interaction between domain pre-training and instruction tuning. The subsequent application of DPO slightly lowers the overall score to 59.6, similar to the pattern observed for Hippo-<img src="static/images/meta.png" class="logo1" alt="Logo" style="width: 20px; height: auto; vertical-align: middle;"/>, with targeted performance adjustment. Based on comprehensive analysis, Mistral 7b + SFT is selected to represent Hippo-<img src="static/images/mistral.png" class="logo2" alt="Logo"  style="width: 18px; height: auto; vertical-align: middle;"/>, credited for its exceptional performance across all benchmarks.
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End Stages <style>.logo1 {vertical-align: middle;}</style>-->

<!-- Uncertainty Quantification -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Uncertainty Quantification</h2>
                  <div class="content has-text-justified">
                    <img src="static/images/uncertain.png" class="interpolation-image" alt="" style="display: block; margin-left: auto; margin-right: auto; width: 95%;"/>
                    <br>
                    <ul style="list-style-type: none;">
                      <li>In our study, we conducted an uncertainty quantification experiment on Hippo-<img src="static/images/mistral.png" class="logo2" alt="Logo"  style="width: 18px; height: auto; vertical-align: middle;"/> to understand its performance on the MedMCQA, MedQA, and PubMedQA datasets, as shown in figure above. Our findings reveal that our model consistently assigns higher probabilities to questions it answers correctly across all datasets, suggesting an ability to self-calibrate its certainty. The model‚Äôs confidence is notably higher on MedMCQA, possibly reflecting the dataset's relative simplicity. In contrast, its confidence on PubMedQA is comparatively lower, likely due to the dataset's complexity. Additionally, the model's confidence changes with different training stages; CPT leads to more conservative estimates, SFT boosts confidence, and adding DPO leads to variable confidence, with noticeable effects in MedMCQA and MedQA. These outcomes emphasize a complex relationship between training approaches and confidence calibration in the model.
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End Uncertainty Quantification -->

<!-- 
<section class="section" id="Conclusion">
  <div class="container is-max-desktop content">
      <h2 class="title">Conclusion</h2>
      <p>In this study, we have introduced Hippocrates, a comprehensive and open-source framework tailored for the medical domain, addressing a wide array of challenges faced by medical LLMs. We provide openly available datasets and establish an intuitive benchmark using the LM-Evaluation-Harness tool. We also introduce Hippo-M and Hippo-L , two 7B models demonstrating superior performance. Our work makes substantial contributions to the field by combining in-depth empirical research with a structured training methodology, offering invaluable insights and tools for future research not only in healthcare but in any area requiring domain-specific adaptation of LLMs</p>
  </div>
</section>
-->


<!-- Limitations -->

<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column has-text-centered">
                  <h2 class="title is-3">Limitations and Safety</h2>
                  <div class="content has-text-justified">
                    <ul style="list-style-type: none;">
                        <li><strong>Model Limitations.</strong> While our 7B model has achieved state-of-the-art results within its class, it is important to acknowledge its limitations compared to larger models such as OpenAI's GPT4. The constraints imposed by the smaller parameter size may impede the model's reasoning capabilities, a crucial aspect of complex medical decision-making. Additionally, the model's performances are almost half on the average which highlights a huge area for improvement in open-source models. <br> <br>
                        <li><strong>Safety and Risks.</strong> Crucially, despite these advancements, it is important to highlight that these AI models need substantial improvements before they can be safely and effectively employed with real patients. They are not yet at a stage where they can provide medical advice or be utilized for commercial healthcare applications. This limitation highlights the need for ongoing, careful development and validation of AI systems to guarantee their reliability and safety in clinical settings. The path toward AI integration in patient care is still unfolding, and while it holds promise, it requires a methodical and thoroughly evaluated approach.
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>
<!-- End Limitations -->


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      Please don't forget to kindly cite our paper if you use our models, data, codes, or results:
      <br><br>
      <pre><code>
        @misc{acikgoz2024hippocrates,
          title={Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare}, 
          author={Emre Can Acikgoz and Osman Batur ƒ∞nce and Rayene Bench and Arda Anƒ±l Boz and ƒ∞lker Kesen and Aykut Erdem and Erkut Erdem},
          year={2024},
          eprint={2404.16621},
          archivePrefix={arXiv},
          primaryClass={cs.LG}
        }
      </code></pre>
  </div>
</section>
<!--End BibTex citation -->

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgements</h2>
      This work is supported in part provided by the KUIS AI Center. The numerical calculations reported in this paper were fully/partially performed at TUBITAK ULAKBIM, High Performance and Grid Computing Center (TRUBA resources). Last but not least, we also acknowledge VSB ‚Äì Technical University of Ostrava, IT4Innovations National Supercomputing Center, Czech Republic, for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (grant ID: 90254)
  </div>
</section>


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
